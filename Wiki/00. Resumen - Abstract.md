# Resumen

El presente trabajo se centra en el desarrollo de una aplicación de escritorio que permite la interacción con modelos de lenguaje de gran tamaño (LLMs) de forma local, combinando accesibilidad, personalización y eficiencia. La solución propuesta utiliza Ollama como motor de ejecución local, eliminando la dependencia de servicios en la nube, garantizando así privacidad y control sobre los datos almacenados.

El sistema se concibe como una herramienta productiva con una interfaz moderna basada en el SDK de aplicaciones de Windows, a través de la cual los usuarios pueden realizar conversaciones o ediciones asistidas por IA, con la posibilidad de configurar agentes personalizados para resumir, traducir o mejorar textos. Además, facilita la gestión de modelos de IA de forma sencilla, permitiendo su búsqueda y descarga sin necesidad de conocimientos avanzados, e incluye un diálogo emergente de edición rápida que mejora la experiencia del usuario. Adicionalmente, se ofrece la opción de un entorno híbrido, almacenando las conversaciones y notas en local, pero utilizando servicios de IA en la nube.

En cuanto al marco teórico, el proyecto incluye documentación sobre el estado del arte de la IA generativa, su historia, una introducción técnica a su funcionamiento y un análisis de la competencia, identificando los factores diferenciadores del proyecto frente a herramientas similares. También se abordan los aspectos sociales, éticos y ambientales de la inteligencia artificial generativa, así como el papel de proyectos como este en la democratización del acceso a modelos de código abierto.

En conclusión, este trabajo presenta una solución que integra la IA en el flujo de trabajo del usuario, sin comprometer la privacidad ni la seguridad de los datos. Con su enfoque modular y escalable, la aplicación sienta las bases para futuras extensiones y mejoras, presentándose como una herramienta clave para la productividad asistida por IA en entornos locales.

# Abstract

The present work focuses on the development of a desktop application that allows interaction with large language models (LLMs) locally, combining accessibility, customization, and efficiency. The proposed solution uses Ollama as a local execution engine, eliminating the dependency on cloud services, thus ensuring privacy and control over stored data.

The system is conceived as a productive tool with a modern interface based on the Windows application SDK, through which users can perform AI-assisted conversations or edits, with the possibility of configuring custom agents to summarize, translate, or enhance texts. Additionally, it facilitates the management of AI models in a simple way, allowing their search and download without the need for advanced knowledge, and includes a quick edit pop-up dialog that improves the user experience. Additionally, it offers the option of a hybrid environment, storing conversations and notes locally, but using AI services in the cloud.

Regarding the theoretical framework, the project includes documentation on the state of the art of generative AI, its history, a technical introduction to its functioning, and a competition analysis, identifying the differentiating factors of the project compared to similar tools. It also addresses the social, ethical, and environmental aspects of generative artificial intelligence, as well as the role of projects like this in democratizing access to open-source models.

In conclusion, this work presents a solution that integrates AI into the user's workflow without compromising data privacy or security. With its modular and scalable approach, the application lays the foundation for future extensions and improvements, presenting itself as a key tool for AI-assisted productivity in local environments.