## 2.1. Conceptos de IA Generativa

### 2.1.1. Historia y Evolución de la IA Generativa

La Inteligencia Artificial Generativa ha tenido un proceso evolutivo largo y fascinante, marcado por hitos importantes en el aprendizaje automático y la creación de distintos modelos computacionales. Desde los primeros pasos en las décadas de 1950 y 1960, hasta los sofisticados modelos actuales basados en transformadores (_transformers_), la IA Generativa ha redefinido profundamente cómo producimos contenido textual, visual y multimedia, automatizando procesos y ampliando las capacidades creativas.

A lo largo de este recorrido, tecnologías clave como las redes neuronales recurrentes (RNNs), las redes generativas antagónicas (GANs) o los _transformers_ han sido responsables de algunos de los mayores avances en esta disciplina. Cada una de estas etapas ha supuesto innovaciones significativas que han aumentado las posibilidades de aplicación de esta tecnología.

#### De los 50 a los 2000s: Primeros pasos

```mermaid
%%{init: { 'theme': 'neutral' } }%%
timeline
    1950 : Inicio de la investigación en redes neuronales
    1960 : Primeros algoritmos de aprendizaje automático
    1990 : Avances en RNNs y autoencoders
    2000 : Introducción al aprendizaje profundo
```

-   **1950: Inicio de la investigación en redes neuronales**

    Surge la idea de emular el cerebro humano mediante algoritmos artificiales. Los primeros modelos, como el perceptrón, establecen la base para el desarrollo de redes más complejas.

-   **1960: Primeros algoritmos de aprendizaje automático**

    Se elaboran los primeros algoritmos diseñados para aprender patrones a partir de muestras de datos, un hito crucial para el aprendizaje automático. Destacan conceptos como descenso del gradiente.

-   **1990: Avances en RNNs y _autoencoders_**

    Las redes neuronales recurrentes (RNNs) permitieron manejar datos secuenciales, como texto y audio. Por otro lado, los _autoencoders_ introdujeron formas eficientes de compresión y reconstrucción de datos, imprescindibles en aplicaciones como el reconocimiento de imágenes.

-   **2000: Introducción al aprendizaje profundo**

    Con el aumento de la capacidad computacional y la disponibilidad de grandes volúmenes de datos, se hicieron populares las arquitecturas de redes de aprendizaje profundo (_Deep Learning_). Algoritmos como el _Deep Belief Network_ y las primeras aplicaciones de convoluciones en visión por computadora (_CNNs_) abren el camino para las redes modernas.

#### De 2011 a 2020: Surgen los modelos generativos

```mermaid
%%{init: { 'theme': 'neutral' } }%%
timeline
    2014 : Introducción de las GANs por Ian Goodfellow
    2017 : Publicación de "Attention is All You Need", introducción de los transformadores
    2019 : Generación de texto avanzado con GPT2
    2020 : Modelos como DALL-E generan contenido visual coherente
```

-   **2014: Introducción de las GANs por Ian Goodfellow**

    Las Redes Generativas Antagónicas (GANs) fueron una revolución para la generación de contenido al utilizar un par de redes que compiten entre sí: una generadora y una discriminadora.

-   **2017: Publicación de "Attention is All You Need", introducción de los transformadores**

    Este artículo presentó el modelo de transformadores, basado en mecanismos de atención que priorizan las relaciones contextuales de los datos. Este hito marcó un antes y un después en el procesamiento de lenguaje natural (NLP).

-   **2019: Generación de texto avanzado con GPT2**

    _OpenAI_ introduce GPT-2, un modelo con capacidad para generar texto coherente y relevante en diferentes contextos. Este modelo mostró el verdadero potencial de la arquitectura _transformer_ para tareas de generación de texto a gran escala.

-   **2020: Modelos como _DALL-E_ generan contenido visual coherente**

    _DALL-E_ (Open AI) combina el procesamiento del lenguaje y la generación de imágenes para crear representaciones visuales basadas en descripciones, mostrando por primera vez una integración única entre texto y contenido visual generado.

#### De 2021 a la actualidad: El BOOM de la IA Generativa

```mermaid
%%{init: { 'theme': 'neutral' } }%%
timeline
    2021 : GPT-3 establece nuevos estándares en generación de texto
    2022 : Auge de modelos open source en IA generativa
    2023 : Popularización de Small Language Models (SLMs)
    2024 : Modelos que razonan, IA más explicativa y precisa
```

-   **2021: GPT-3 establece nuevos estándares en generación de texto**

    GPT-3, con 175 mil millones de parámetros, lleva las capacidades de generación de texto a un nivel nunca visto antes, destacando por su habilidad para contextualizar, interpretar y crear contenido coherente y específico.

-   **2022: Auge de modelos open source en IA generativa**

    Modelos como **BLOOM** (BigScience) o **Stable Diffusion** (Runway) contribuyeron a la democratización de la IA generativa, ofreciendo alternativas personalizables y locales a las soluciones comerciales, una aportación importante para la investigación. Muchos otros modelos open source surgieron desde entonces.

-   **2023: Popularización de Small Language Models (SLMs)**

    Los SLMs, como **TinyLlama** y otras variantes compactas de **LLaMA**, o **Phi-2** (Microsoft), ganaron popularidad por su capacidad de ofrecer buenos resultados con menor consumo de recursos. Se popularizaron técnicas como la destilación del conocimiento, que permiten entrenar modelos pequeños a partir de modelos grandes, reteniendo gran parte de su capacidad a un menor coste.

-   **2024: Modelos que razonan: IA explicativa y precisa**

    Modelos como **o1** (OpenAI) y avances en **Claude 3** (Anthropic) producen un punto de inflexión en el razonamiento contextual y explicable, ya que no solo generan contenido, sino que emplean tiempo en razonar de forma interna antes de responder. Estas soluciones representan un paso importante hacia contenidos más seguros y confiables.

>Sources:
>https://www.qualcomm.com/news/onq/2024/02/>the-rise-of-generative-ai-timeline-of-breakthrough-innovations
>https://www.datacamp.com/blog/top-small-language-models + https://arxiv.org/pdf/>2409.15790
>https://timeline.the-blueprint.ai/
>https://nhlocal.github.io/AiTimeline/

### 2.1.2. Principios de los Modelos Generativos

Los modelos generativos se basan en aprender patrones y características a partir de datos existentes para luego crear contenido nuevo que sea coherente y relevante. A lo largo de los años, se han desarrollado distintas técnicas, arquitecturas y enfoques que han marcado hitos importantes en este campo. En este apartado, veremos algunos de los conceptos más destacados.

#### Tokens

Los _tokens_ son la unidad mínima de texto que procesa un determinado modelo de lenguaje. Dependiendo del enfoque, un token puede representar una palabra completa, una parte de una palabra (por ejemplo, "auto", "re", "mente") o incluso una sola letra o signo. La forma en que se realiza esta segmentación puede influir mucho en la eficiencia y precisión del modelo. Por ejemplo, en arquitecturas como los *transformers*, optimizar la segmentación de tokens resulta esencial para que el modelo entienda el contexto de forma adecuada y no se pierda en secuencias demasiado largas o ambiguas.

#### Embeddings

Para que los modelos generativos puedan "entender" datos como texto o incluso imágenes, es necesario convertir esa información en representaciones numéricas (vectores). En el caso del lenguaje, esto se logra a través de los llamados _embeddings_. Estas son básicamente representaciones vectoriales de palabras o tokens, donde cada dimensión refleja alguna característica semántica o contextual. En otras palabras, si dos palabras tienen significados similares, sus embeddings estarán más "cerca" en el espacio vectorial.

(Posible diagrama con diferentes métodos de tokenización / mismo texto en fragmentos distintos)

#### Transformers

Un _transformer_ es un modelo o arquitectura de aprendizaje profundo desarrollado por científicos trabajadores de _Google_ y publicados en el artículo de investigación _Attention is All You Need_. Su novedad reside en el mecanismo de atención, que permite detectar y priorizar las partes más relevantes del texto de entrada. Gracias a este enfoque, se logra una comprensión del contexto mucho más profunda que con arquitecturas anteriores.

(diagrama de atención de un transformer para aclarar cómo funciona).

#### Redes Generativas Antagónicas (GANs)

Las Redes Generativas Antagónicas están formadas por dos redes neuronales que trabajan de forma competitiva: el generador produce contenido (por ejemplo, una imagen), mientras que el discriminador intenta distinguir si lo que ve es real o generado. Esta especie de juego de "tira y afloja" impulsa la red generadora a mejorar, resultando en creaciones cada vez más realistas. Gracias a esta técnica, se ha dado un salto enorme en la calidad de imágenes y videos sintéticos.

#### Aprendizaje por Difusión

El aprendizaje por difusión se basa en ir transformando gradualmente información estructurada mediante la difusión de ruido aleatorio (datos sin sentido), para luego revertir ese proceso "limpiando" la información y generando imágenes de mayor calidad. Es una técnica que permite un gran control sobre el resultado, sobre todo cuando se genera una imagen a partir de una descripción de texto.

#### Fine-Tuning

El _fine-tuning_ aprovecha el conocimiento adquirido por un modelo preentrenado y lo adapta a un dominio o tarea específica, reduciendo costos y tiempo de entrenamiento en comparación con empezar desde cero.

En el desarrollo de software, el _fine-tuning_ adquiere gran relevancia al facilitar la personalización de modelos generativos para necesidades puntuales, como automatizar flujos de trabajo empresariales, diseñar interfaces a medida o codificar las reglas de negocio de una forma específica, convirtiéndose así en un recurso esencial para incorporar la IA generativa como asistente activo en cada fase del desarrollo.

(ilustración del concepto fine-tuning)

>Sources:
>https://www.ibm.com/mx-es/think/topics/diffusion-models
>https://www.ibm.com/es-es/topics/knowledge-distillation

## 2.2. Funcionamiento de la IA Generativa

### 2.2.1. La arquitectura Transformer

La aparición de la arquitectura Transformer marcó un antes y un despuesta en el ámbito del procesamiento del lenguaje natural. Hasta entonces, los sistemas "leían" los textos de forma limitada, por palabras, y los embeddings resultantes (vectores asociados a cada palabra) tenían poca información del contexto. Los transformers son un tipo especial de red neuronal que, gracias al **mecanismo de atención**, añaden un alto nivel de información contextual a los vectores, aumentando enormemente la precisión de las operaciones semánticas que podemos realizar con los mismos.

![Ilustración de una operación semántica: Obtener la capital de un país.](./Pictures/1_LhgyIESTI58f_5Hb8sCSFg.webp)

[_Fuente: Wikipedia_](https://en.wikipedia.org/wiki/Word_embedding)

Con el **mecanismo de atención** es como si todas las palabras de un texto se miraran a unas las otras y se preguntaran a cuál debe prestar más atención, esto ocurre de forma paralela con todas las palabras procesadas, como si todas se sentasen en una mesa redonda y hablasen entre ellas. Cada vector ya no representa solo a un token independiente, sino que posee un poco de información de cuánto ese token se relaciona con cada palabra del contexto.

![Ejemplo del modelo de atención representado como un grafo](./Pictures/image5.png)

(fuente: https://research.google/blog/constructing-transformers-for-longer-sequences-with-sparse-attention-methods/)

#### Del embedding al texto: propabilidad y muestro

Tras pasar por múltiples capas, los embeddings finales se proyectan con una capa lineal y se aplica una función **softmax** para obtener una distribución de probabilidad sobre el vocabulario. Este proceso es la base de la **generación de texto**: cada token generado se selecciona considerando su probabilidad. Así podemos completar un texto a partir de información puramente estadística.

(fuente: [llm-class.github.io](https://llm-class.github.io/slides/Lecture%206%20-%20The%20Transformer%20Architecture%20-%20Part%20II.pdf?utm_source=chatgpt.com).)

![Ejemplo de cómo se predice la siguiente palabra de una frase](./Pictures/Pasted-image-20250608171727.png)

Fuente https://textsynth.com/completion.html

Los principales parámetros para la generación son:

- **Temperatura** (T): Se aplica reescalando las probabilidades. Si T < 1, la distribución se concentra, favoreciendo tokens altamente probables y haciendo el texto más determinista. Si T > 1, se “aplana” la distribución y aumenta la aleatoriedad.
- **Top‑p** (muestreo del núcleo o *nucleus sampling*): Se selecciona el menor número de tokens que conforman un acumulado de probabilidad de al menos p y luego se normalizan esas probabilidades. Esto garantiza que solo se consideran los tokens más plausibles, pero sin limitarse a un conjunto fijo.

Además, existen otros parámetros como la _repetition penalty_ o la _presence penalty_, que ayudan a evitar repeticiones innecesarias y fomentan la diversidad en la salida, ajustando aún más el comportamiento creativo o técnico del modelo según el caso de uso.

![Ejemplo del funcionamiento del parámetro Top-p](./Pictures/Pasted-image-20250608175825.png)

#### Características de los modelos generativos

Los modelos generativos como los basados en Transformers se caracterizan principalmente por su **número de parámetros**, en este caso en referencia a la cantidad de pesos y conexiones que se ajustan durante su entrenamiento. Este número es clave porque determina la capacidad del modelo para aprender patrones complejos: modelos pequeños pueden tener decenas de millones de parámetros, mientras que los más avanzados llegan a cientos de miles de millones. Es importante no confundir estos “parámetros del modelo” con los parámetros de generación como temperatura o top-p, ya que los primeros definen la arquitectura y el conocimiento aprendido, mientras que los segundos controlan el comportamiento durante la generación de texto.

Otra característica fundamental es el **tamaño de la ventana de contexto** (_context window_), que indica cuántos tokens puede “ver” el modelo a la vez para tomar decisiones. Una ventana pequeña limita la cantidad de información reciente que el modelo puede tener en cuenta, lo que afecta la coherencia en textos largos (el modelo empezará a "olvidar"). Por eso, los modelos modernos han ido ampliando este límite, permitiendo mantener conversaciones más largas o procesar documentos extensos de una sola vez, mejorando notablemente la experiencia de usuario.

En los últimos años, junto con el crecimiento de los modelos, ha surgido un ecosistema de modelos **open source** y **open weight**, que ofrecen acceso al conocimiento aprendido en sus parámetros. Un modelo open weight publica sus pesos entrenados, pero no necesariamente su código fuente. Por otro lado, un modelo open source proporciona los pesos, el código y detalles sobre el proceso de entrenamiento, lo que permite estudiarlo, adaptarlo y reentrenarlo según las necesidades. Esta apertura ha democratizado el acceso a los grandes modelos de lenguaje, incluyendo los avances en la generación local de texto, permitiendo a usuarios y organizaciones mantener el control sobre sus datos y personalizar los modelos de acuerdo a sus propios casos de uso, sin renunciar a las capacidades y ventajas de la arquitectura Transformer.

### 2.2.2. Generación Conversacional

Los modelos de lenguaje se entrenaron inicialmente para generar texto libre a partir de una entrada (prompt), pero para mantener interacciones coherentes y útiles en conversaciones la mayoría de modelos generativos actuales también **son entrenados con datos estructurados en forma de diálogos**. En lugar de usar simplemente cadenas de texto planas, estos entrenamientos utilizan plantillas o _templates_ que representan las conversaciones como secuencias de mensajes con roles definidos, lo que ayuda al modelo a entender el contexto conversacional, identificar quién está hablando y responder de forma adecuada según su papel.

Cuando enviamos una conversación al modelo, esta se codifica siguiendo una plantilla interna que incluye **tokens especiales de control**. Por ejemplo, en los modelos basados en el formato de OpenAI, Mistral o LLaMA, se usan tokens como `<|start|>`, `<|user|>`, `<|assistant|>` o similares, dependiendo del tokenizer y el modelo. Una versión simplificada de cómo se convierte una conversación para el modelo podría ser algo así:

```
<|system|>
Eres un asistente útil.
<|user|>
¿Cuál es la capital de Francia?
<|assistant|>
...
```

A medida que avanzamos en una conversación, esta plantilla va repitiendo el patrón, intercalando los mensajes de `user` y `asistantes` y finalizando con el mensaje del usuario más reciente. Este texto es el que realmente se tokeniza y se utiliza en el proceso de inferencia (se le pide al modelo que lo complete).

La plantilla y los tokens exactos dependen del modelo específico y su *tokenizador*, que aunque muchas veces no son públicos, siguen un esquema similar. Esto es lo que permite al modelo "saber" quién habla y qué tipo de respuesta se espera. Los roles más comunes utilizados en estas plantillas son:

- **system**: Define el contexto general o las instrucciones para toda la conversación. Suele estar presente al inicio y no cambia durante el intercambio.
- **user**: Representa lo que escribe el usuario. Puede ser una pregunta, una orden, o simplemente una afirmación.
- **assistant**: Contiene la respuesta generada por el modelo. Este es el rol que se quiere predecir en la mayoría de los casos.

#### Estandarización a través del API de OpenAI

Aunque no existe un estándar oficial universal para este tipo de estructuración, lo cierto es que el API de OpenAI (en especial el endpoint `chat/completions`) se ha convertido en una especie de estándar de facto. Su forma de representar conversaciones y el contrato que utiliza ha sido adoptado, o replicado con variaciones mínimas, por otros proveedores como Azure OpenAI, Google Gemini, Mistral, o incluso implementaciones locales como Ollama.

Este es un ejemplo típico de llamada a un endpoint `chat/completions` *OpenAI-compatible*.

```json
{
    "messages": [
        { "role": "system",    "content": "Eres un asistente útil."         },
        { "role": "user",      "content": "¿Cuál es la capital de Francia?" },
        { "role": "assistant", "content": "La capital de Francia es París." },
        { "role": "user",      "content": "¿Y a qué distancia está de Madrid?" }
    ],
    "model": "mistral-small-latest",
    "temperature": 0.5
}
```

Este hecho ha permitido que muchos frameworks y bibliotecas de cliente puedan trabajar con múltiples proveedores simplemente cambiando la URL o la clave de API, sin tener que reescribir la lógica de integración. Este grado de compatibilidad facilita mucho tanto el desarrollo de herramientas como la experimentación con distintos modelos, lo que beneficia especialmente a proyectos de código abierto o con recursos limitados.

![Representación de la colección de mensajes enviados al modelo de IA generativa, su tarea es generar (o predecir) cómo debe continuar la conversación.](Pasted-image-20250610193959.png)

## 2.2.3. IA Generativa Local

Uno de los aspectos clave para ejecutar modelos de lenguaje en local es entender cómo deben prepararse para que funcionen de forma eficiente en dispositivos personales. A diferencia del entorno en la nube, donde se dispone de una gran cantidad de recursos, en local tenemos que optimizar tanto el tamaño como el rendimiento del modelo sin comprometer demasiado su capacidad de generación de texto. Para lograr esto, existen distintos formatos y técnicas que permiten empaquetar y adaptar los modelos de forma adecuada.
#### Técnicas de optimización

-   **Quantización**:

    Uno de los pasos más importantes en la preparación de modelos para uso local es la _quantización_. Esta técnica consiste en reducir la precisión numérica de los pesos del modelo (por ejemplo, pasar de 32 bits en coma flotante a 4 u 8 bits). Aunque podría parecer que esto degrada la calidad del modelo, en la práctica se logra un buen equilibrio entre eficiencia y rendimiento.  
    Los tipos de cuantización más comunes incluyen `Q8_0` / `Q8_K`, que utilizan 8 bits y mantienen una alta precisión, pero con una notable reducción del tamaño y del consumo de memoria; `Q6_K` / `Q5_K`, que ofrecen un punto intermedio con buena calidad y menor peso; y `Q4_0` / `Q4_K` / `Q2_K`, que son más agresivos y están pensados para dispositivos con recursos muy limitados.
    
-   **Destilación**:  

    La _destilación_ es otra técnica muy utilizada para optimizar modelos de lenguaje. Consiste en entrenar un modelo más pequeño (con menos menos parámetros) a partir del comportamiento de uno grande. En vez de aprender directamente de los datos originales, el modelo más pequeño (llamado "estudiante") aprende a imitar las respuestas del modelo más grande ("profesor"). Esto permite obtener modelos mucho más ligeros, que mantienen buena parte del rendimiento original, pero con tiempos de carga e inferencia mucho menores.
    
Gracias a la combinación de **quantización** y **destilación**, es posible ejecutar modelos que originalmente ocupaban decenas de gigas (como LLaMA o Mistral) en una laptop con 8 o 16 GB de RAM, manteniendo tiempos de respuesta razonables y consumo moderado de CPU/GPU. Estas optimizaciones son clave para hacer viable el uso de IA generativa en local sin necesidad de infraestructura especializada.

#### Formatos más comunes para modelos locales

Existen varios formatos utilizados para almacenar modelos de lenguaje optimizados para ejecución local. Entre los más conocidos están:

- **GGUF** (GPT Grammar Unified Format): Es el formato que utilizado en Ollama. Se trata de una evolución del antiguo formato `GGML`, diseñado específicamente para soportar modelos **cuantizados** y listos para ejecución eficiente en CPU y GPU sin necesidad de bibliotecas pesadas. GGUF incluye metadatos estructurados, soporte para múltiples quantizaciones, y compatibilidad con herramientas como llama.cpp. Su diseño modular lo hace ideal para distribución y carga rápida.

- **Safetensors**: Aunque es más común en entornos como Hugging Face, este formato seguro y binario también puede ser utilizado en local, especialmente en proyectos donde se usa PyTorch o similares.

- **ONNX** (Open Neural Network Exchange): Formato abierto y optimizado para inferencia multiplataforma, compatible con muchos runtimes (como ONNX Runtime, DirectML o TensorRT). Se usa más en contextos industriales o cuando se busca portabilidad entre sistemas.

En el ámbito de este proyecto utilizaremos modelos en formato **GGUF**, preparados para inferencia local y obtenidos principalmente a través de la biblioteca de **Ollama**, aunque también podremos utilizar modelos convertidos disponibles en **Hugging Face**.

## 2.3. Aplicaciones de IA Generativa

### 2.3.1. Aplicaciones Conversacionales

Las aplicaciones conversacionales han transformado profundamente la forma en la que los usuarios interactúan con la tecnología. Desde los sistemas básicos de respuesta automática hasta los actuales asistentes avanzados impulsados por modelos de lenguaje, su evolución ha sido marcada por la búsqueda de interacciones más naturales, personalizadas y eficientes. Hoy en día, estas aplicaciones no solo generan texto, sino que también comprenden el contexto, ejecutan acciones específicas y permiten generar y editar contenido de forma colaborativa.
#### Definición y Contexto Histórico

Las aplicaciones conversacionales son sistemas diseñados para interactuar mediante lenguaje natural, ya sea escrito o hablado. Sus primeros antecedentes se remontan a los años 60 con programas como **ELIZA**, un chatbot que simulaba a una psicoterapeuta mediante reglas simples y respuestas predefinidas. Sin embargo, estos sistemas eran limitados y carecían de comprensión real.

El verdadero salto cualitativo ocurrió con la introducción de técnicas avanzadas de **aprendizaje profundo** y la aparición de arquitecturas como **transformers**, que revolucionaron el procesamiento del lenguaje natural. Gracias a estos avances, hoy en día existen diversas aplicaciones conversacionales que abarcan desde asistentes virtuales, chatbots de atención al cliente, e incluso empiezan a surgir interfaces conversacionales integradas en diversas aplicaciones como Copilot dentro del ecosistema Microsoft.

#### Principales Aplicaciones Web Actuales

-   **ChatGPT (OpenAI)**: Ha sido clave en la democratización del acceso a los modelos generativos avanzados. Sus capacidades de razonamiento contextual permiten responder preguntas complejas, ofrecer soporte técnico y facilitar la creación de contenido creativo.

-   **Gemini (Google)**: En su intento por competir en el mercado, Google ha desarrollado un modelo que no solo genera texto, sino que también se integra con el ecosistema de aplicaciones de Google. Esto facilita tareas como búsquedas mejoradas o asistencia en la edición de documentos.

-   **Claude (Anthropic)**: Este asistente destaca por su enfoque en la **seguridad y la ética**, con capacidades diseñadas para minimizar la generación de respuestas erróneas o perjudiciales. Su razonamiento explicativo permite ofrecer respuestas justificadas y confiables.

-   **Otras aplicaciones**: Otras herramientas similares que han cobrado importancia recientemente son DeepSeek (startup China que impresionó al mundo con un modelo de razonamiento de bajo coste en enero de 2025), herramientas conversacionales integradas en aplicaciones como GitHub Copilot (integrada en herramientas de programación), Copilot de Microsoft, Grok de xAI o la francesa Mistral.

#### Características Esenciales de las Aplicaciones Conversacionales Modernas

1.  **Conversación Dinámica y Persistente**

    Las aplicaciones actuales son capaces de mantener conversaciones fluidas y recordar el contexto. Por ejemplo, en herramientas como ChatGPT, es posible retomar temas anteriores sin necesidad de repetir información, lo que mejora la coherencia y la experiencia del usuario.

2.  **Agentes Especializados**

    Un aspecto clave es la creación de agentes personalizados mediante **prompts predefinidos**. Estos agentes son configuraciones del modelo adaptadas a tareas concretas mediante instrucciones específicas, como el análisis de textos legales, la detección de errores gramaticales, la generación de resúmenes, traducción de textos, etc.

    *(Incluir un diagrama mostrando cómo un único modelo puede generar múltiples agentes a partir de diferentes instrucciones)*

3.  **Interfaces de Edición Conversacional**

    Inspiradas en herramientas como **Copilot** y **Notion AI**, estas interfaces van más allá del chat tradicional, permitiendo al usuario interactuar directamente sobre documentos. Por ejemplo, se pueden solicitar correcciones, ampliaciones de contenido, mejora en el formato.

4.  **Búsqueda y Acceso a Bases de Conocimiento**

    Los sistemas modernos se integran con diversas fuentes externas, como bases de datos internas o búsquedas web. Este tipo de integración es esencial en entornos empresariales, donde la disponibilidad de información relevante puede agilizar procesos críticos.

#### Nuevas Tendencias

* **Razonamiento y Ediciones Contextualizadas**

La investigación en modelos avanzados, como **o3** y **o4** (OpenAI) y **DeepSeek R1**, sugiere que las futuras aplicaciones conversacionales no se limitarán a generar respuestas inmediatas. En cambio, incorporarán capacidades de razonamiento previas a la generación del texto, lo que permitirá ofrecer respuestas más precisas y argumentadas. Además, los sistemas de edición colaborativa en tiempo real harán que la interacción entre usuarios y modelos sea más fluida y personalizada.

* **Privacidad con Aplicaciones Locales**

Impulsados por una demanda cada vez mayor por parte de los usuarios por tener un mayor control sobre sus datos y garantizar la privacidad, las aplicaciones conversacionales locales están experimentando últimamente un gran crecimiento. A diferencia de los servicios en la nube, estas aplicaciones permiten ejecutar los modelos de lenguaje directamente en ordenadores y dispositivos domésticos, eliminando la necesidad de transmitir información personal a servidores externos. Este enfoque proporciona beneficios claros: desde una **confidencialidad reforzada** hasta la eliminación de la dependencia de una conexión a internet, ya que estas aplicaciones se ejecutan **fuera de línea**.

En los últimos años, han surgido proyectos innovadores que han sentado las bases para el desarrollo de aplicaciones conversacionales locales, como **llama.cpp**, un proyecto que ha sido fundamental para permitir la **ejecución eficiente de grandes modelos** de lenguaje, como **LLaMA**, en hardware común. Por otro lado, **Ollama** se destaca por proporcionar un interfaz sencillo para gestionar modelos locales mediante líneas de comando o llamadas API, simplificando el proceso de integración, convirtiéndose en una alternativa atractiva para desarrolladores.

Además, la aparición de **NPUs** _(Neural Processing Units)_ en ordenadores personales y dispositivos móviles está acelerando esta transformación tecnológica. Diseñadas para optimizar el procesamiento de tareas relacionadas con la IA, las NPUs ofrecen una ejecución más eficiente y menos demandante en cuanto a consumo energético frente a los procesadores convencionales. Empresas como **Apple**, **Intel**, **AMD** y **Qualcomm**, ya han integrado esta tecnología. Este avance no solo mejora el rendimiento, sino que también refuerza la privacidad y personalización en aplicaciones conversacionales, señalando una evolución significativa hacia la adopción local de inteligencia artificial.

Todas estas novedades abren camino a una nueva tendencia para aplicaciones conversacionales que desean combinar personalización, rendimiento, privacidad y compatibilidad con la ejecución fuera de línea.

### 2.3.2. Revisión de Soluciones Existentes

La evolución de los modelos de lenguaje a gran escala ha dado lugar a la aparición de numerosas herramientas y plataformas que permiten su uso en distintos entornos. Desde soluciones en la nube hasta herramientas que posibilitan la ejecución de modelos localmente, cada una ofrece ventajas y limitaciones según el caso de uso. En esta sección, se presentan las principales soluciones actuales, su enfoque y una comparativa de sus capacidades en términos de privacidad, accesibilidad y funcionalidad.
#### Herramientas Conversacionales en la Nube

Las plataformas en la nube han dominado el espacio de la inteligencia artificial generativa debido a su accesibilidad y escalabilidad. Estas soluciones permiten a los usuarios interactuar con modelos de lenguaje sin necesidad de preocuparse por la infraestructura, ya que el procesamiento se realiza en servidores remotos.

Algunas de las soluciones más destacadas incluyen:

-   **ChatGPT (OpenAI)**: Modelo basado en la arquitectura GPT, accesible desde la web y aplicaciones móviles. Ofrece respuestas contextuales avanzadas, acceso a modelos recientes y funciones de memoria en la versión Plus.
-   **Gemini (Google)**: Alternativa de Google con integración en su ecosistema, capaz de realizar tareas avanzadas como la búsqueda en tiempo real y el análisis de información en documentos.
-   **Claude (Anthropic)**: Diseñado con un enfoque en la seguridad y la ética, optimizado para respuestas confiables y explicaciones claras.
-   **DeepSeek (DeepSeek)**: Modelo de código abierto con alto rendimiento en razonamiento y matemáticas. Destaca por su contexto largo, capacidad multilingüe y acceso gratuito.
-   **Copilot (Microsoft)**: Herramienta de asistencia integrada en aplicaciones de Microsoft 365, diseñada para mejorar la productividad mediante sugerencias inteligentes y automatización de tareas rutinarias.

Además de soluciones más recientes como MetaAI con Llama 4 (integrado en la popular aplicación móvil Whatsapp), Grok de xAI (integrada en la red social X, anteriormente Twitter), Le Chat (la aplicación conversación de la francesa Mistral), entre muchas otras. Si bien estos modelos ofrecen capacidades avanzadas, dependen completamente de la conectividad a internet y pueden plantear preocupaciones sobre la privacidad de los datos del usuario.

![El ecosistema de aplicaciones conversacionales en la nube es cada vez más grande](./Pictures/chatgpt-is-my-favorite-which-one-is-your-favorite-and-go-to-v0-thc7jnwvpule1.webp)

#### Herramientas para Ejecutar LLMs Localmente

La necesidad de privacidad y autonomía ha impulsado el desarrollo de herramientas que permiten ejecutar modelos de IA en dispositivos locales. La mayoría de estas soluciones se basan en **llama.cpp**, un framework en C/C++ optimizado para la ejecución de modelos en hardware doméstico. Sobre esta base, surgió **Ollama**, que “eleva” a llama.cpp al ofrecer una gestión simplificada de modelos, una API de alto nivel y funciones adicionales (como la configuración de un servidor local) que facilitan la integración con diversos clientes e interfaces de usuario. En esencia, Ollama actúa como un envoltorio o “*wrapper*” que abstrae la complejidad técnica de llama.cpp para que los desarrolladores puedan enfocarse en la interacción con la IA sin preocuparse por los detalles de bajo nivel.
##### Interfaces para Ollama

-   **Open WebUI**: Proporciona una interfaz rica en funciones para ejecutar modelos locales con integración de técnicas como RAG y búsqueda web.
-   **Enchanted**: Aplicación nativa para macOS con integración optimizada para sistemas Apple.
-   **Hollama**: Interfaz web minimalista, que busca ofrecer una opción simple que puede ejecutarse dentro del navegador sin instalación.
-   **LibreChat**: Un clon de ChatGPT de código abierto que también ofrece instalación y configuración como interfaz web.

##### Otras herramientas basadas en llama.cpp

-   **LM Studio**: Aplicación de escritorio enfocadas en gestionar y ejecutar modelos en formato GGUF, con opciones de personalización y compatibilidad con múltiples arquitecturas, no es de código abierto.
-   **Msty**: Muy similar a LM Studio, también de código propietario.
-   **GPT4All**: Proporciona una experiencia similar a ChatGPT, completamente local, con modelos optimizados para ejecución en CPU y GPU.
-   **Jan.ai**: Alternativa open source con una interfaz optimizada. Utiliza cortex.cpp, un fork propio de llama.cpp.
-   **AnythingLLM**: Otra opción open-source interesante basada en llama.cpp.

##### Herramientas de Productividad de Escritorio

Las herramientas de IA no solo se limitan a la generación de texto, sino que también han sido integradas en flujos de trabajo para mejorar la productividad y la automatización. Algunas de las soluciones más utilizadas incluyen:

-   **Microsoft Copilot 365**: Extiende la IA a herramientas como Word, Excel y Teams, ofreciendo asistencia en redacción, análisis de datos y generación de contenido.
-   **GitHub Copilot**: Asistente de programación basado en modelos de OpenAI, integrado en entornos de desarrollo para sugerencias de código en tiempo real.
-   **ChatGPT Desktop**, **Microsoft Copilot** o **Claude Desktop**: Son versiones de escritorio de sus respectivas aplicaciones web, con interfaces muy similares a la tan popular web de ChatGPT. Totas ellas cuentan normalmente con funcionalidades muy limitadas si no se adquiere una suscripción de pago.

![Microsoft Copilot](./Pictures/Pasted-image-20250608162757.png)

Estas son solo algunas herramientas diseñadas para mejorar la eficiencia en distintos ámbitos, pero dependen de la conectividad a la nube y pueden presentar limitaciones en cuanto a la personalización y privacidad, sobre todo en las modalidades sin pago de subscripción.

### 3.3.4. Comparativa de Enfoques, Ventajas y Limitaciones

Frente a las soluciones existentes, el proyecto que proponemos integra en una sola aplicación múltiples funcionalidades que actualmente no están combinadas en ningún competidor. Entre sus características diferenciadoras se incluyen:

-   **Cliente de escritorio sencillo y accesible**, sin necesidad de configuraciones complejas.
-   **Gestión de conversaciones y edición de notas enriquecida**, permitiendo una interacción fluida con modelos de IA dentro de un bloc de notas.
-   **Soporte completo para la ejecución y gestión de modelos con Ollama**, facilitando la descarga y actualización de modelos locales.
-   **Compatibilidad con servidores privados o en la nube**, permitiendo al usuario elegir un enfoque mixto, con almacenamiento de conversaciones y notas en local, pero utilizando procesamiento en red privada o en la nube.
-   **Accesos directos a agentes preconfigurados**, que pueden ejecutarse sin salir del contexto actual, optimizando la productividad mediante un diálogo de edición rápida.

Con esta propuesta, se busca ofrecer una alternativa equilibrada entre **potencia, privacidad y facilidad de uso**, adaptándose a diferentes necesidades sin imponer limitaciones tecnológicas o de acceso.